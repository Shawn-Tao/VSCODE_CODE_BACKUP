# 最优化专题

## SLAM经典模型

$$
    \left\{
        \begin{aligned}
        x_k = f(x_{k-1},u_k)+\omega_k \\
        z_{k,j} = h(y_j,x_k)+\upsilon_{k,j}
        \end{aligned}
    \right.
$$
此处 $x_k$ 为相机的位姿,可以用 $SE(3)$ 来描述.  
观测方程,第五讲已经说明,为针孔相机模型.  
假设在 $x_k$ 处对路标 $y_k$ 进行了一次观测,对应到图像上的像素位置为 $z_{k,j}$,那么观测方程为
$$
sz_{j,k} = K(R_k y_j + t_k)
$$
其中

- K为相机内参
- s为像素点距离,也是 $(R_k y_j + t_k)$ 第三个分量

考虑噪声对数据的影响,在上述两方程中引入,两个噪声项 $\omega_k,\:\upsilon_{k,j}$ 假设它们满足零均值的高斯分布,如下
$$
\begin{aligned}
    \omega_k \sim \mathcal{N} (0,R_k) \\
    \upsilon \sim \mathcal{N} (0,Q_{k,j})
\end{aligned}
$$
其中 $R_k\:Q_{k,j}$ 为协方差矩阵  
关于高斯分布和协方差矩阵知识点可才参考以下博客  
[<font size = 5>高斯分布</font>](https://blog.csdn.net/LoseInVain/article/details/80339201)  [<font size = 5>协方差矩阵</font>](https://blog.csdn.net/qq_23869697/article/details/80610361) 
   
在这些噪声印象下,我们希望通过带噪声的数据 $z$ 与 $u$ 推断位姿 $x$ 与地图 $y$ (以及他们的概率分布)  
<font color = #FF6197 size = 4> 这构成一个概率估计问题</font><br/>
处理这个概率估计问题大致两种思路:增量(渐进,滤波器)的方法,或批量的方法

## 批量方法建模

概率学观点来看,已知 输入数据 $u$ 与 $z$ 的情况下,求状态 $x$, $y$ 的条件概率分布
$$
P(x,y|z,u)
$$
为了估计状态变量的条件分布,利用贝叶斯法则,又
$$
P(x,y\mid z,u) = \frac{P(z,u\mid x,y) P(x,y)}{P(z,u)} \propto \underbrace{P(z,u\mid x,y)}_{likelihood} \underbrace{P(x,y)}_{prior}
$$
贝叶斯法则左侧称为后验概率,右侧分别为似然与先验  
直接求后验分布式困难的,但是求一个状态最优估计,使得该状态下后验概率最大则是可行的:
$$
(x,y)^*_{MAP} = \arg \max P (x,y\mid z,u) = \arg \max P(z,u\mid x,y)P(x,y)
$$
$\arg \max$代表函数值最大时,自变量的取值  
由于贝叶斯法则分母部分与待估计状态 $x,y$ 无关,因而可以忽略  
贝叶斯法则启示我们,求解最大后验概率等价于最大似然和先验的乘积  
如果不知道机器人位姿或路标大概在什么地方,此时就没有了先验,那么可以求解最大似然估计(MLE):
$$
(x,y)^* = arg max P(z,u \mid x,y)
$$

## 最小二乘法引出

$$
z_{k,j} = h(y_j,x_k) + \upsilon
$$
由于我们假设了噪声项服从高斯分布 $\upsilon \sim \mathcal{N} (0,Q_{k,j})$, 所以观测数据地条件概率为
$$
P(z_{j,k} \mid x_k,y_j) = N(h(y_j,x_k),Q_{k,j})
$$
它显然是一个高斯分布  
考虑到单次观测的最大似然估计,可以使用最小化负对数来求一个高斯分布的最大似然  
$$
P(x) = \frac{1}{\sqrt{(2\pi^N \det(\Sigma))}} \exp (-1/2(x-\mu)^T\Sigma^{-1} (x-\mu))
$$
取其负对数
$$
-\ln (P(x)) = \frac{1}{2} \ln((2\pi)^N \det(\Sigma)) + \frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)
$$
对数函数是单调递增的,所以对原函数求最大化,相当于负对数最小化
略去与 $x$ 无关项,整理得到所求为
$$
(x_k,y_j)^* = \arg \min ((z_{i,j} - h(x_k,y_j))^T Q_{k,j}^{-1} (z_{k,j} - h(x_k,y_j)))
$$
我们发现,该式等价于最小化噪声项(即误差)的一个二次型,这个二次型被称为马哈拉诺比斯距离,又称为马氏距离.它也可以看成 $Q_{k,j}^{-1}$ 加权后的欧氏距离,这里 $Q_{k,j}^{-1}$ 也叫做信息矩阵,即高斯分布协方差矩阵之逆  
假设各个时刻的输入和观测是相互独立的，则可以对联合分布式进行因式分解，得到
$$
p(z,u \mid x,y) = \prod_k P(u_k \mid x_{k-1},x_k) \prod_{k,j} P(z_{k,j}|x_k,y_j)
$$
$$
\begin{aligned}
    e_{u,k} &= x_k - f(x_{k-1},u_k) \\
    e_{z,j,k} &= z_{k,j} - h(x_k,y_j)
\end{aligned}
$$
那么最小化所有时刻估计值与真实读数之间的马氏距离,等价于求最大似然估计,负对数允许我们把乘积变成求和
$$
\min J(x,y) = \sum_k e_{u,k}^T R_k^{-1}e_{u,k} + \sum_k \sum_j e_{z,k,j}^T Q_{k,j}^(-1)e_{z,k,j}
$$
这就得到了最小二乘问题,但由于噪声的存在,当我们把估计的轨迹与地图带入SLAM运动,观测方程中,他们并不会完美成立  
因而此处借助非线性优化的思维,对状态的估计值进行微调,使整体误差下降一些,不断调节,直到下降值为一个极小值.  

## 最小二乘法求解 

对于简单形式最小而成问题,如
$$
\min_x F(x) = \frac{1}{2} \mid \mid f(x) \mid \mid_2^2
$$
其中,$x \in R^n$ $f$ 是任意非线性函数 $f(x): R^n \rightarrow R$ 这里的系数 $\frac{1}{2}$ 是无关紧要的  
显然,如果 $f$ 的数学形式很简单,那么可以用解析的方式直接进行求解:  
令目标函数的导数为零,然后求解 $x$ 的最优值,就和求二元函数的极值一样
$$
\frac{dF}{dx} = 0
$$
解此方程,就得到了导数为零的极值,可能是极大,极小或鞍点,逐个比较函数值大小即可  
实际问题中,导函数形式往往复杂,也难以知道目标函数的全局性质.对于不方便直接求解的最小二乘问题,我们采用迭代的方式,从一个初始值出发,不断更新当前优化变量  
这让解导函数为零的问题,变成了一个不断寻找下降增量 $\Delta x_k$ 的问题  
当函数下降到增量很小时,认为算法收敛,目标函数达到了一个极小值.  
在这个过程中,每次计算我们只关心f在迭代处的局部性质.  
以下我们介绍一些常用方法

### 一阶和二阶梯度法(牛顿法)

考虑第 $k$ 次迭代 ,假设我们在 $x_k$ 处,想要寻找增量 $\Delta x_k$ 最直观的方式是将目标函数在 $x_k$ 附近进行<font color = #FF6197 size = 3>泰勒展开</font><br/>
$$
F(x_k + \Delta x_k) \approx F(x_k) + J(x_k)^T \Delta x_k + \frac{1}{2} \Delta x_k^T H(x_k) \Delta x_k
$$
其中 $J(x_k)$ 是 $F(x)$ 关于 $x$ 的一阶导数
<font color = #FF6197 size = 3> 也叫梯度或Jacobian矩阵 将其写作列向量,则可以与 $\Delta x$ 进行内积,得到一个标量</font><br/>
而 $H$ 则是二阶导数(Hessian矩阵),他们都在$x_k$处取值  
如果保留一阶梯度,那么取增量为反向的梯度,则可保证函数下降
$$
\Delta x^* = -J(x_k)
$$
当然这只代表方向,我们还需要指定一个步长(可按照一定条件计算,联想机器学习)
如果保留二姐信息,此时增量方程为
$$
\Delta x^* = \arg \min (F(x) + J(x)^T \Delta x + \frac{1}{2} \Delta x^T H \Delta x)
$$
求右侧关于 $\Delta x$ 导数并令其等于零,则有
$$
J + H \Delta x = 0 \Leftarrow H \Delta x = -J
$$
求解这个方程得到增量,称为牛顿法
以上方法存在的缺点:

1. 最速下降法过于贪心,容易走出锯齿状路线,增加迭代次数
2. 牛顿法需要计算目标函数Hessian矩阵,在问题规模较大时非常困难

### 高斯牛顿法

我们会看目标函数
$$
\min_x F(x) = \frac{1}{2} \mid \mid f(x) \mid \mid_2^2
$$
区别于牛顿法处理 $F(x)$ 高斯牛顿法是将 $f(x)$ 进行一阶泰勒展开得到
$$
f(x+ \Delta x) \approx f(x) + J(x)^T \Delta x
$$
这里的 $J(x)^T$ 是关于 $x$ 的导数  
根据前面的框架,目前目标是寻找一个 $\Delta x$ 使得 $\mid\mid f(x) + \Delta x \mid\mid$ 达到最小,为了求 $\Delta x$ 我们需要解一个线性最小二乘问题
$$
\Delta x^* = \arg \min_{\Delta x} \frac{1}{2} \mid \mid f(x) + J(x)^T \Delta x \mid \mid^2
$$
展开平方项,求关于 $\Delta x$ 导数并令其等于零,得到
$$
J(x)f(x) + J(x)J^T(x) \Delta x = 0
$$
可以得到如下方程组
$$
\underbrace{J(x)J^T(x)}_{H(x)} \Delta x = \underbrace{-J(x)f(x)}_{g(x)}
$$
这个方程是关于变量 $\Delta x$ 的线性方程组,我们称之为增量方程,也可以称为高斯牛顿方程或者正规方程.  
我们把左边的系数定义为 $H$, 右边定义为 $g$,那么上式就变成
$$
H \Delta x = g
$$
这里把左侧记作 $H$ 是有意义的,对比牛顿法,高斯牛顿将 $JJ^T$ 作为牛顿法中二阶Hessian矩阵的近似,从而省略计算 $H$ 的过程  
为了求解增量方程,我们需要求解 $H^-1$ 这需要 $H$ 矩阵可逆,但实际求解出的 $J J^T$ 只具有半正定性,这可能导致算法不收敛,一定程度上克服高斯牛顿法缺点,以下介绍列文伯格-马夸尔特方法

### 列文伯格-马夸尔特方法

二阶泰勒展开仅在展开点附近有较好的近似效果,所以应当给 $\Delta x$ 添加一个范围,称为信赖区域,定义了在什么情况下二阶近似是有效的,这个方法也称为信赖区域方法在此采用以下指标刻画近似的好坏程度
$$
\rho = \frac{f(x+\Delta x)-f(x)}{J(x)^T\Delta x}
$$
如果 $\rho$ 接近于1,则近似是好的!  
如此问题转化为
$$
\min_{\Delta x_k} \frac{1}{2} \mid \mid f(x_k) + J(x_k)^T \Delta x_k \mid \mid^x,\:s.t. \mid \mid D \Delta x_k\mid \mid^2 \leq \mu
$$
利用拉个朗日乘子法(KKT条件)进行分析,存在以下约束
$$
\left\{
\begin{aligned}
    &(\Delta x_k, \lambda) = \frac{1}{2} \mid \mid f(x_k) + J(x_k)^T \Delta x_k \mid \mid^2 + \frac{\lambda}{2} (\mid \mid D\Delta x_k \mid \mid^2 - \mu) \\
    &\lambda (\mid \mid D \Delta x \mid \mid^2 - \mu) = 0 \\
    &\lambda > 0
\end{aligned}
\right.
$$
实际工作中,我们主要关心算法是否顺利工作,因而仅计算约束中第一项,当信赖条件不满足,或目标函数增加,就增加 $\lambda$ 的权重,直到满足条件  
令拉格朗日函数关于 $\Delta x$ 导数为零,得到如下线性方程
$$
(H + \lambda D D^T) \Delta x_k = g
$$
相比于高斯牛顿法,增量方程多了一项 $\lambda D D^T$ ,如需简化,可以考虑 D = I,得到如下
$$
(H + \lambda I) \Delta x_k = g
$$

### 需要明确的事项

1. 以上的方法,进行最优化计算的时候,都需要提供良好的初始值!实际问题中,我们会采用 ICP或PnP之类的算法提供优化初始值
2. 常常采用矩阵分解方式进行线性方程组 $\Delta x$的解决,例如 QR分解,Cholesky分解等
3. 视觉SLAM问题中,矩阵往往具有特定的稀疏形式,降低了求解难度

### Ceres进行曲线拟合

Ceres求解最小二乘法问题最一般的形式如下(带边界的核函数最小二乘)
$$
\begin{aligned}
    &\min_x \frac{1}{2} \sum_i \rho_i (\mid \mid f_i (x_i1,...,x_in)\mid \mid^2)\\
    &s.t.\:l_j<x_j<u_j
\end{aligned}
$$
这个问题中$x_i1,...,x_in$ 为优化变量,又称为参数块儿  
$f_i$ 称为代价函数(cost function),也称为残差块(Residual blocks),可以理解为SLAM中的误差项  
$l_j$ 和 $u_j$  为第j个优化变量的上限与下限,最简单的情况下,$l_j = -\infty u_j = \infty$(不限制优化变量的边界),此时目标函数由许多平方项经过一个和函数 $\rho()$ 之后求和组成  
同样,可以取 $\rho$ 为恒等函数,那么目标函数极为许多项的平方和,我们就得到了无约束的最小二乘问题  

#### 使用Ceres准备工作

1. 定义每个参数块(通常为平凡向量,也可定义为四元数或李代数等结构,如果是向量,那么需要为每一个参数块分配一个double数组)
2. 定义残差计算方式()
3. 残差块往往也需要定义雅各比计算方式(可以调用自己的方法,也可以手动指定雅各比计算过程)
4. 把所有参数块和残差块加入Ceres定义的Prmblem对象中,调用solve函数求解

### 使用g2o进行曲线拟合(General Graphic Optimization)

图优化知识此处不做介绍,之后尽量深入学习